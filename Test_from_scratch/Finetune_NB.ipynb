{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import own modules\n",
    "import config\n",
    "from utils_dir import transforms \n",
    "\n",
    "#empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "###############Dataloader for training the model####################\n",
    "from DL_finetune import ESC_50_DL_finetune as DSf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# contrastive triplet model with resnet50 without dropout\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten for easier downstream processing\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./results/CLR-2023-09-26-12-epochs-300\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super(FineTuneModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Insert input size of the encoder here 2048\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = config.finetune_epochs\n",
    "learning_rate = config.lr\n",
    "weight_decay = 1e-5  # L2 regularization\n",
    "batch_size = 32\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 60  # This value can be changed based on how many epochs of no improvement you're willing to wait\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Load the entire pre-trained model (from your contrastive training)\n",
    "pretrained_model = torch.load(log_dir + '/checkpoint.pth')\n",
    "encoder_trained = pretrained_model.encoder\n",
    "\n",
    "# Initialize the FineTuneModel with the pre-trained encoder\n",
    "num_classes = 50  # Adjust this to the number of classes in your dataset\n",
    "model = FineTuneModel(encoder_trained, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./finetune_results/FineTune-{current_date}-epochs-{num_epochs}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "# Variables for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for _, (file_name, data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop (You may need to modify this to fit your specific use-case)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, (file_name, data, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Save model if validation loss improves\n",
    "        # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Saving the model...\")\n",
    "        torch.save(model, log_dir + '/checkpoint.pth')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:11<00:00,  6.24s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss improved! Saving the model...\n",
      "Epoch [1/400], Train Loss: 4.4145, Val Loss: 4.5998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:02<00:00,  6.06s/it]\n",
      "100%|██████████| 13/13 [00:12<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss improved! Saving the model...\n",
      "Epoch [2/400], Train Loss: 4.4100, Val Loss: 4.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:00<00:00,  6.01s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/400], Train Loss: 4.4226, Val Loss: 4.6019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:05<00:00,  6.10s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss improved! Saving the model...\n",
      "Epoch [4/400], Train Loss: 4.4176, Val Loss: 4.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:31<06:03,  7.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gabriel\\OneDrive\\Dokumente\\GitHub\\MA-pre-work\\Test_from_scratch\\Finetune_NB.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gabriel/OneDrive/Dokumente/GitHub/MA-pre-work/Test_from_scratch/Finetune_NB.ipynb#X12sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gabriel/OneDrive/Dokumente/GitHub/MA-pre-work/Test_from_scratch/Finetune_NB.ipynb#X12sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gabriel/OneDrive/Dokumente/GitHub/MA-pre-work/Test_from_scratch/Finetune_NB.ipynb#X12sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gabriel/OneDrive/Dokumente/GitHub/MA-pre-work/Test_from_scratch/Finetune_NB.ipynb#X12sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m avg_train_loss \u001b[39m=\u001b[39m train_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gabriel/OneDrive/Dokumente/GitHub/MA-pre-work/Test_from_scratch/Finetune_NB.ipynb#X12sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Validation loop\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = config.finetune_epochs\n",
    "learning_rate = 5e-4#config.lr\n",
    "weight_decay = 1e-5  # L2 regularization\n",
    "batch_size = 32\n",
    "num_classes = 50  # Adjust this to the number of classes in your dataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 60  # This value can be changed based on how many epochs of no improvement you're willing to wait\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Load the entire pre-trained model (from your contrastive training)\n",
    "pretrained_model = torch.load(log_dir + '/checkpoint.pth')\n",
    "encoder_trained = pretrained_model.encoder\n",
    "\n",
    "# Initialize the FineTuneModel with the pre-trained encoder\n",
    "model = FineTuneModel(encoder_trained, num_classes).to(device)\n",
    "\n",
    "# One-hot encoding and custom loss function\n",
    "def hotEncoder(v):\n",
    "    ret_vec = torch.zeros(v.shape[0], num_classes).to(device)\n",
    "    for s in range(v.shape[0]):\n",
    "        ret_vec[s][v[s]] = 1\n",
    "    return ret_vec\n",
    "\n",
    "def cross_entropy_one_hot(input, target):\n",
    "    _, labels = target.max(dim=1)\n",
    "    return nn.CrossEntropyLoss()(input, labels)\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./finetune_results/FineTune-{current_date}-epochs-{num_epochs}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "# Variables for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for _, (file_name, data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        label_vec = hotEncoder(labels)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = cross_entropy_one_hot(outputs, label_vec)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, (file_name, data, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            label_vec = hotEncoder(labels)\n",
    "            outputs = model(data)\n",
    "            loss = cross_entropy_one_hot(outputs, label_vec)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Save model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Saving the model...\")\n",
    "        torch.save(model, log_dir + '/checkpoint.pth')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for (file_name, data, labels) in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = sum(t == p for t, p in zip(true_labels, pred_labels))\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for (file_name, data, labels) in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Convert lists to arrays for better indexing and operations\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = np.sum(true_labels == pred_labels)\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
