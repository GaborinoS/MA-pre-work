{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import own modules\n",
    "import config\n",
    "from utils_dir import transforms \n",
    "\n",
    "#empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "###############Dataloader for training the model####################\n",
    "from DL_finetune import ESC_50_DL_finetune as DSf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# contrastive triplet model with resnet50 without dropout\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=False)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten for easier downstream processing\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./results/CLR-2023-09-23-14-epochs-100\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super(FineTuneModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Insert input size of the encoder here 2048\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = config.finetune_epochs\n",
    "learning_rate = config.lr\n",
    "weight_decay = 1e-5  # L2 regularization\n",
    "batch_size = 32\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # This value can be changed based on how many epochs of no improvement you're willing to wait\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Load the entire pre-trained model (from your contrastive training)\n",
    "pretrained_model = torch.load(log_dir + '/checkpoint.pth')\n",
    "encoder_trained = pretrained_model.encoder\n",
    "\n",
    "# Initialize the FineTuneModel with the pre-trained encoder\n",
    "num_classes = 50  # Adjust this to the number of classes in your dataset\n",
    "model = FineTuneModel(encoder_trained, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./finetune_results/FineTune-{current_date}-epochs-{num_epochs}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "# Variables for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for _, (file_name, data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop (You may need to modify this to fit your specific use-case)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, (file_name, data, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Save model if validation loss improves\n",
    "        # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Saving the model...\")\n",
    "        torch.save(model, log_dir + '/checkpoint.pth')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:10<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.25%\n",
      "Precision: 12.38%\n",
      "Recall: 18.25%\n",
      "F1-score: 12.43%\n",
      "Confusion Matrix:\n",
      "[[4 1 0 ... 0 0 0]\n",
      " [1 2 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 3 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for (file_name, data, labels) in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = sum(t == p for t, p in zip(true_labels, pred_labels))\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
