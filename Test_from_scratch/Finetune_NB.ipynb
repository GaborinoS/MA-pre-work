{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim, nn\n",
    "\n",
    "#own modules\n",
    "import config\n",
    "from utils_dir.utils import *\n",
    "\n",
    "#empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "###############Dataloader for training the model####################\n",
    "if config.ADSMI:\n",
    "    from DL_finetune import ADSMI_DL_finetune as DSf\n",
    "    Data_name = 'ADSMI'\n",
    "if config.ESC_50:\n",
    "    from DL_finetune import ESC_50_DL_finetune as DSf\n",
    "    Data_name = 'ESC-50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# contrastive triplet model with resnet50 without dropout\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten for easier downstream processing\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048, projection_dim=128,input_channels=config.channels):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=False)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(input_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),  # 1st projection layer, can be modified\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, projection_dim)  # 2nd projection layer\n",
    "        )\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for easier downstream processing\n",
    "        x = self.projection(x)  # Pass through the projection head\n",
    "        #x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set here the Model that should be fintuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./results/CLR-2023-10-06-12-epochs-300-ESC_50\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super(FineTuneModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Insert input size of the encoder here 2048\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = config.finetune_epochs\n",
    "learning_rate = config.lr\n",
    "weight_decay = 1e-5  # L2 regularization\n",
    "batch_size = 32\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 60  # This value can be changed based on how many epochs of no improvement you're willing to wait\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Load the entire pre-trained model (from your contrastive training)\n",
    "pretrained_model = torch.load(log_dir + '/checkpoint.pth')\n",
    "encoder_trained = pretrained_model.encoder\n",
    "\n",
    "# Initialize the FineTuneModel with the pre-trained encoder\n",
    "num_classes = 50  # Adjust this to the number of classes in your dataset\n",
    "model = FineTuneModel(encoder_trained, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./finetune_results/FineTune-{current_date}-epochs-{num_epochs}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "# Variables for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for _, (file_name, data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop (You may need to modify this to fit your specific use-case)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, (file_name, data, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Save model if validation loss improves\n",
    "        # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Saving the model...\")\n",
    "        torch.save(model, log_dir + '/checkpoint.pth')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "num_epochs = config.finetune_epochs\n",
    "learning_rate = config.lr\n",
    "weight_decay = 1e-5  # L2 regularization\n",
    "batch_size = config.batch_size\n",
    "num_classes = config.class_numbers  # Adjust this to the number of classes in your dataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = config.patience  # This value can be changed based on how many epochs of no improvement you're willing to wait\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Load the entire pre-trained model (from your contrastive training)\n",
    "pretrained_model = torch.load(log_dir + '/checkpoint.pth')\n",
    "encoder_trained = pretrained_model.encoder\n",
    "\n",
    "# Initialize the FineTuneModel with the pre-trained encoder\n",
    "model = FineTuneModel(encoder_trained, num_classes).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Assuming the WarmUpExponentialLR class and config values are available\n",
    "scheduler = WarmUpExponentialLR(optimizer, cold_epochs= 0, warm_epochs= config.warm_epochs, gamma=config.gamma)  # Adjust warm_epochs and gamma as needed\n",
    "\n",
    "\n",
    "# One-hot encoding and custom loss function\n",
    "def hotEncoder(v):\n",
    "    ret_vec = torch.zeros(v.shape[0], num_classes).to(device)\n",
    "    for s in range(v.shape[0]):\n",
    "        ret_vec[s][v[s]] = 1\n",
    "    return ret_vec\n",
    "\n",
    "def cross_entropy_one_hot(input, target):\n",
    "    _, labels = target.max(dim=1)\n",
    "    return nn.CrossEntropyLoss()(input, labels)\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./finetune_results/FineTune-{current_date}-epochs-{num_epochs}-{Data_name}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "#write in log file the hyperparameters\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "    log_file.write(f\"##############################################################################\\n\")\n",
    "    log_file.write(f\"Hyperparameters:\\n\")\n",
    "    if config.ADSMI:\n",
    "        log_file.write(f\"ADSMI labeled Data\\n\")\n",
    "    if config.ESC_50:\n",
    "        log_file.write(f\"ESC-50 labeled Data\\n\")\n",
    "\n",
    "    log_file.write(f\"num_epochs: {num_epochs}\\n\")\n",
    "    log_file.write(f\"initial_learning_rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"weight_decay: {weight_decay}\\n\")\n",
    "    log_file.write(f\"batch_size: {batch_size}\\n\")\n",
    "    log_file.write(f\"patience: {patience}\\n\")\n",
    "    log_file.write(f\"early_stop_counter: {early_stop_counter}\\n\")\n",
    "    log_file.write(f\"num_classes: {num_classes}\\n\")\n",
    "    #log_file.write(f\"model: {model}\\n\")\n",
    "    log_file.write(f\"criterion: CrossEntropyLoss()\\n\")\n",
    "    #log_file.write(f\"optimizer: {optimizer}\\n\")\n",
    "    #log_file.write(f\"scheduler: {scheduler}\\n\")\n",
    "    #log_file.write(f\"log_dir: {log_dir}\\n\")\n",
    "    log_file.write(f\"log_file_path: {log_file_path}\\n\")\n",
    "    #log_file.write(f\"train_loader: {train_loader}\\n\")\n",
    "    #log_file.write(f\"test_loader: {test_loader}\\n\")\n",
    "    log_file.write(f\"##############################################################################\\n\")\n",
    "\n",
    "# Variables for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for _, (file_name, data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        label_vec = hotEncoder(labels)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = cross_entropy_one_hot(outputs, label_vec)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _, (file_name, data, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            label_vec = hotEncoder(labels)\n",
    "            outputs = model(data)\n",
    "            loss = cross_entropy_one_hot(outputs, label_vec)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Saving the model...\")\n",
    "\n",
    "        # Log to file\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(f\"Validation Loss improved! Saving the model...\\n\")\n",
    "        \n",
    "        torch.save(model, log_dir + '/checkpoint.pth')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    #scheduler step update the learning rate  \n",
    "    scheduler.step()         \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Log to file early stoping counter\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Early stopping counter: {early_stop_counter} from {patience}\\n\")\n",
    "\n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "        log_file.write(f\"##\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for (file_name, data, labels) in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = sum(t == p for t, p in zip(true_labels, pred_labels))\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.25%\n",
      "Precision: 80.22%\n",
      "Recall: 77.25%\n",
      "F1-score: 76.89%\n",
      "Confusion Matrix:\n",
      "[[8 0 0 ... 0 0 0]\n",
      " [0 6 0 ... 0 0 0]\n",
      " [0 0 7 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 6 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model = torch.load('finetune_results/FineTune-2023-09-30-17-epochs-400/checkpoint.pth')\n",
    "model.eval()\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_loader, test_loader = DSf.create_generators_finetune()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for (file_name, data, labels) in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Convert lists to arrays for better indexing and operations\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = np.sum(true_labels == pred_labels)\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
