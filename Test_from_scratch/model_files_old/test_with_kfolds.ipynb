{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.models as models\n",
    "import torchaudio\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# Implement Stratified K-Folds Cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_freq,\n",
    "        resample_freq,\n",
    "        device,\n",
    "        n_fft=2048,\n",
    "        hop_length = 512,\n",
    "        n_mels=80,  \n",
    "        win_length = 2048,\n",
    "        window = 'hann',\n",
    "        desired_length_in_seconds=5,\n",
    "        train=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train = train\n",
    "        self.desired_length_in_seconds = desired_length_in_seconds\n",
    "        self.sample_rate = 32000\n",
    "        self.mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=32000,  # Your sample rate\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        win_length=win_length,\n",
    "        window_fn=torch.hann_window  # This corresponds to the 'hann' window\n",
    "        ).to(device)\n",
    "\n",
    "        self.PS = T.PitchShift(self.sample_rate, n_steps=1.1).to(device)\n",
    "        \n",
    "        self.amplitude = T.AmplitudeToDB().to(device)\n",
    "\n",
    "        self.spec_aug = torch.nn.Sequential(\n",
    "            T.TimeStretch(random.uniform(0.8, 1.2), fixed_rate=True).to(device),\n",
    "            T.FrequencyMasking(freq_mask_param=15).to(device),\n",
    "            T.TimeMasking(time_mask_param=90).to(device),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random_crop_or_pad(waveform, sample_rate, desired_length_in_seconds=5):\n",
    "            \"\"\"\n",
    "            Randomly crops the waveform to the desired length in seconds.\n",
    "            If the waveform is shorter than the desired length, it will be padded with zeros.\n",
    "            \"\"\"\n",
    "            desired_length = desired_length_in_seconds * sample_rate\n",
    "            current_length = waveform.shape[1]\n",
    "\n",
    "            # If the waveform is shorter than desired, pad it with zeros\n",
    "            side = random.randint(0,2)\n",
    "\n",
    "            if current_length < desired_length:\n",
    "                if side == 0:\n",
    "                    padding_needed = desired_length - current_length\n",
    "                    left_pad = padding_needed // 2\n",
    "                    right_pad = padding_needed - left_pad\n",
    "                    waveform = torch.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "                elif side == 1:\n",
    "                    padding_needed = desired_length - current_length\n",
    "                    left_pad = padding_needed\n",
    "                    right_pad = 0\n",
    "                    waveform = torch.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "                else:\n",
    "                    padding_needed = desired_length - current_length\n",
    "                    left_pad = 0\n",
    "                    right_pad = padding_needed\n",
    "                    waveform = torch.nn.functional.pad(waveform, (left_pad, right_pad))\n",
    "            \n",
    "            # Calculate the starting point for cropping\n",
    "            start_idx = random.randint(0, waveform.shape[1] - desired_length)\n",
    "            return waveform[:, start_idx:start_idx+desired_length]\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Apply pitch shift\n",
    "        #waveform = self.PS(waveform)\n",
    "\n",
    "        #crop or pad\n",
    "        waveform = self.random_crop_or_pad(waveform, 32000, self.desired_length_in_seconds)\n",
    "\n",
    "        # Convert to power spectrogram\n",
    "        spec = self.mel_spectrogram(waveform)\n",
    "\n",
    "        # Apply SpecAugment\n",
    "        if self.train: spec = self.spec_aug(spec)\n",
    "        \n",
    "        # Convert to decibel\n",
    "        spec = self.amplitude(spec).squeeze(0)\n",
    "\n",
    "\n",
    "        if config.channels == 3:\n",
    "            spec = torch.stack([spec, spec, spec]) \n",
    "\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3776\n",
      "Val size:  944\n"
     ]
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "labels_file = pd.read_csv('./data/labeled_ADSMI/labels_int.csv', index_col=0)\n",
    "labels_file = labels_file.drop(columns=['fold'])\n",
    "train_df, val_df = train_test_split(labels_file, test_size=0.2, stratify=labels_file['Label_int'], random_state=42)\n",
    "# train test split\n",
    "print(\"Train size: \", len(train_df))\n",
    "print(\"Val size: \", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MyDataset_finetune(Dataset,):\n",
    "    \n",
    "    def __init__(self, train_indices=None, test_indices=None, train=True, sample_rate=32000, desired_length_in_seconds=10):\n",
    "        self.root = './data/labeled_ADSMI/labeled_data_2013-535/'\n",
    "        self.train = train\n",
    "        \n",
    "        #getting name of all files inside the all of the train_folds\n",
    "        temp = os.listdir(self.root)\n",
    "        temp.sort()\n",
    "        self.file_names = []\n",
    "        self.class_ids = []\n",
    "\n",
    "        if train_indices is not None:\n",
    "            self.file_names = labels_file.iloc[train_indices][\"filename\"].values\n",
    "            self.class_ids = labels_file.iloc[train_indices][\"Label_int\"].values\n",
    "\n",
    "        if test_indices is not None:\n",
    "            self.file_names = labels_file.iloc[test_indices][\"filename\"].values\n",
    "            self.class_ids = labels_file.iloc[test_indices][\"Label_int\"].values\n",
    "\n",
    "        \n",
    "        if self.train:\n",
    "            self.pipeline = MyPipeline(sample_rate, sample_rate, 'cuda', desired_length_in_seconds=desired_length_in_seconds, train=self.train)\n",
    "            self.pipeline.to(device=torch.device(\"cuda\"), dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        else: #for test\n",
    "            self.pipeline = MyPipeline(sample_rate, sample_rate, 'cuda', desired_length_in_seconds=desired_length_in_seconds, train=self.train)\n",
    "            self.pipeline.to(device=torch.device(\"cuda\"), dtype=torch.float32)    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_names[index]  \n",
    "        path = self.root + file_name\n",
    "        \n",
    "        # Using torchaudio to load waveform\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = waveform.to(device=torch.device(\"cuda\"), dtype=torch.float32)\n",
    "\n",
    "        mel_spec = self.pipeline(waveform)\n",
    "\n",
    "        class_id = self.class_ids[index]\n",
    "\n",
    "        return mel_spec, class_id\n",
    "\n",
    "def create_generators_finetune(train_indices=None, test_indices=None):\n",
    "    train_dataset = MyDataset_finetune(train_indices=train_indices, train=True, desired_length_in_seconds=config.desired_length_in_seconds)\n",
    "    test_dataset = MyDataset_finetune(test_indices=test_indices, train=False, desired_length_in_seconds=config.desired_length_in_seconds)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=0 ,drop_last=False)\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size = config.batch_size, shuffle=True, num_workers=0 ,drop_last=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(config.channels)\n",
    "\n",
    "class Resnet50_Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Resnet50_Classifier, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.conv1 = nn.Conv2d(config.channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        num_features = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Fold:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/52 [00:13<01:21,  1.82s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "#------Data fold generation for cross-validation\n",
    "n_folds = 8\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(set(labels_file[\"Label_int\"]))  # Assuming the number of classes is the unique count of \"Label_int\" in your labels_file\n",
    "model = Resnet50_Classifier(num_classes)\n",
    "#model = ModifiedResnet50_Classifier(num_classes)\n",
    "#model = ResNet101_Classifier(num_classes)\n",
    "\n",
    "\n",
    "\n",
    "#  Transfer the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay = 1e-4 ) # Adjust the value as needed)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.8, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "config.best_accuracy = 0\n",
    "config.model_path = \"./results_standalone/test_checkpoint.pth\"\n",
    "\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(skf.split(train_df, train_df['Label_int'])):\n",
    "    \n",
    "    train_loader, test_loader = create_generators_finetune(train_indices=train_indices, test_indices=test_indices)\n",
    "    #  Create an instance of the model\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Fold: \", fold)\n",
    "    num_epochs = 1  # Adjust this as needed\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (spectrograms, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Set the model to evaluation mode (important for dropout and batch normalization)\n",
    "        model.eval()\n",
    "\n",
    "        # Iterate through the test set\n",
    "        with torch.no_grad():  # Disable gradient computation during testing\n",
    "            for spectrograms, labels in test_loader:\n",
    "                # Move data to the testing device\n",
    "                spectrograms = spectrograms.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_losses.append(loss.item())\n",
    "                \n",
    "                # Compute the predicted labels\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                # Update evaluation metrics\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        #if new test accuracy is better than the previous best, save the model\n",
    "        if correct_predictions / total_samples > config.best_accuracy:\n",
    "            config.best_accuracy = correct_predictions / total_samples\n",
    "            torch.save(model, config.model_path)\n",
    "            \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step(test_losses[-1])\n",
    "\n",
    "        # Calculate accuracy or other evaluation metrics\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
