{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import own modules\n",
    "import config\n",
    "from utils_dir import transforms \n",
    "\n",
    "#empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC_50 Loaded for pretraining\n"
     ]
    }
   ],
   "source": [
    "if config.ESC_50:\n",
    "    from DL_pretrain import ESC_50_DL_pretrain as DS\n",
    "    print(\"ESC_50 Loaded for pretraining\")\n",
    "    Data_name = \"ESC_50\"\n",
    "if config.US8K:\n",
    "    from DL_pretrain import US8k_DL_pretrain as DS\n",
    "    print(\"US8K Loaded for pretraining\")\n",
    "    Data_name = \"US8K\"\n",
    "if config.ADSMI:\n",
    "    from DL_pretrain import ADSMI_DL_pretrain as DS\n",
    "    print(\"ADSMI Loaded for pretraining\")\n",
    "    Data_name = \"ADSMI\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = DS.AudioDataset(train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def plot_triplet_spectrogram(train_loader, n_triplets=1):\n",
    "    \"\"\"\n",
    "    Plot spectrograms for anchor, positive and negative from the first batch of the given loader.\n",
    "    \n",
    "    Args:\n",
    "    - train_loader (torch.utils.data.DataLoader): Data loader for triplet data.\n",
    "    \"\"\"\n",
    "    # Fetch the first batch\n",
    "    data_iter = iter(train_loader)\n",
    "    anchor, positive, negative = data_iter.__next__()\n",
    "\n",
    "    # Function to plot spectrogram\n",
    "    def plot_spectrogram(spectrogram, ax, title):\n",
    "        im = ax.imshow(spectrogram.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "        ax.set_title(title)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(n_triplets):\n",
    "        # Print shape of tensors\n",
    "        print(\"Shape of tensors in triplet {0}:\".format(i+1))\n",
    "        print(\"Anchor:\\t\\t\\t{0}\\nPositive:\\t\\t{1}\\nNegative:\\t\\t{2}\".format(anchor[i].shape, positive[i].shape, negative[i].shape))\n",
    "        print(torch.equal(anchor[i], positive[i]))\n",
    "        print(torch.allclose(anchor[i], positive[i], atol=1e-6))\n",
    "\n",
    "        # Plot the first triplet\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        plot_spectrogram(anchor[i], axes[0], \"Anchor\")\n",
    "        plot_spectrogram(positive[i], axes[1], \"Positive\")\n",
    "        plot_spectrogram(negative[i], axes[2], \"Negative\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_triplet_spectrogram(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def rescale_data(data):\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "\n",
    "def plot_triplet_spectrogram(train_loader, n_triplets=1):\n",
    "    \"\"\"\n",
    "    Plot RGB spectrograms for anchor, positive and negative from the first batch of the given loader.\n",
    "    \n",
    "    Args:\n",
    "    - train_loader (torch.utils.data.DataLoader): Data loader for triplet data.\n",
    "    \"\"\"\n",
    "    # Fetch the first batch\n",
    "    data_iter = iter(train_loader)\n",
    "    anchor, positive, negative = data_iter.__next__()\n",
    "\n",
    "    # Function to plot RGB spectrogram\n",
    "    def plot_spectrogram(spectrogram, ax, title):\n",
    "        # Assume spectrogram has shape (3, Height, Width) representing (C, H, W)\n",
    "        # Convert to numpy and transpose to (H, W, C) for visualization\n",
    "        im = ax.imshow(np.transpose(rescale_data(spectrogram.numpy()), (1, 2, 0)))\n",
    "\n",
    "        ax.set_title(title)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    for i in range(n_triplets):\n",
    "        # Print shape of tensors\n",
    "        print(\"Shape of tensors in triplet {0}:\".format(i+1))\n",
    "        print(\"Anchor:\\t\\t\\t{0}\\nPositive:\\t\\t{1}\\nNegative:\\t\\t{2}\".format(anchor[i].shape, positive[i].shape, negative[i].shape))\n",
    "        print(torch.equal(anchor[i], positive[i]))\n",
    "        print(torch.allclose(anchor[i], positive[i], atol=1e-6))\n",
    "\n",
    "        # Plot the first triplet\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        plot_spectrogram(anchor[i], axes[0], \"Anchor\")\n",
    "        plot_spectrogram(positive[i], axes[1], \"Positive\")\n",
    "        plot_spectrogram(negative[i], axes[2], \"Negative\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_triplet_spectrogram(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveTripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5, temperature=0.07):\n",
    "        super(ContrastiveTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - anchor: Embeddings from log_s_po_aug1\n",
    "        - positive: Embeddings from log_s_po_aug2\n",
    "        - negative: Embeddings from log_s_neg_aug1\n",
    "\n",
    "        Returns:\n",
    "        - A loss scalar.\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive) / self.temperature\n",
    "        neg_sim = F.cosine_similarity(anchor, negative) / self.temperature\n",
    "\n",
    "        # Compute the triplet loss\n",
    "        losses = F.relu(self.margin - pos_sim + neg_sim)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Model without dropout layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "# Triplet Model without dropout layer\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=False)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        #print(f\"Shape of x before unsqueeze: {x.shape}\") # diagnostic print\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten for easier downstream processing\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(config.channels)\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048, projection_dim=128):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=False)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(config.channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),  # 1st projection layer, can be modified\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, projection_dim)  # 2nd projection layer\n",
    "        )\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for easier downstream processing\n",
    "        x = self.projection(x)  # Pass through the projection head\n",
    "        #x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model without PH but Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# contrastive triplet model with resnet50 without dropout\n",
    "\n",
    "class ContrastiveTripletModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTripletModel, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Modifications for your dataset:\n",
    "        # Assuming your data is a spectrogram of shape [128, X]. \n",
    "        # ResNet50 expects 3-channel inputs, so let's adapt the first layer.\n",
    "        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove last FC layer to get embeddings\n",
    "        self.encoder = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Dropout layer (with 50% probability, adjust as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten for easier downstream processing\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:39<15:43, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:11<13:30, 35.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:44<12:28, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [02:17<11:43, 33.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [02:49<10:59, 32.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [03:20<10:18, 32.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [03:52<09:38, 32.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [04:23<09:05, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [04:56<08:36, 32.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [05:29<08:04, 32.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [06:01<07:33, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [06:34<07:02, 32.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [07:07<06:30, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [07:38<05:54, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [08:09<05:19, 31.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [08:41<04:47, 31.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [09:13<04:13, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [09:44<03:41, 31.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [10:15<03:08, 31.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [10:46<02:36, 31.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [11:17<02:05, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [11:48<01:33, 31.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [12:19<01:02, 31.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [12:50<00:31, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [13:21<00:00, 32.06s/it]\n",
      "100%|██████████| 7/7 [01:13<00:00, 10.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] Training Loss: 0.2357, Validation Loss: 0.0882\n",
      "Validation Loss improved! Checkpointing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:30<12:16, 30.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:01<11:46, 30.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:34<11:36, 31.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431]) torch.Size([64, 3, 128, 431])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "patience = 30  # or whatever value you deem appropriate\n",
    "early_stop_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "print(config.epochs)\n",
    "print(config.batch_size)\n",
    "\n",
    "\n",
    "# Initialization\n",
    "model = ContrastiveTripletModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "criterion = ContrastiveTripletLoss()\n",
    "\n",
    "# Data\n",
    "train_loader, test_loader = DS.create_generators()\n",
    "\n",
    "epochs = config.epochs\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Create log directory\n",
    "current_date = datetime.datetime.now().strftime('%Y-%m-%d-%H')\n",
    "log_dir = f\"./results/CLR-{current_date}-epochs-{epochs}-{Data_name}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training and Validation loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (anchor, positive, negative) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        print(anchor.shape, positive.shape, negative.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_repr, positive_repr, negative_repr = model(anchor, positive, negative)\n",
    "\n",
    "        loss = criterion(anchor_repr, positive_repr, negative_repr)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchor, positive, negative) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "\n",
    "            anchor_repr, positive_repr, negative_repr = model(anchor, positive, negative)\n",
    "            loss = criterion(anchor_repr, positive_repr, negative_repr)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Log to file\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(\"Validation Loss improved! Checkpointing the model...\")\n",
    "        torch.save(model, os.path.join(log_dir, f\"checkpoint.pth\"))  # Modified this line\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(f\"Validation Loss improved at Epoch {epoch+1}.\\n\")\n",
    "        early_stop_counter = 0  # reset counter\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        \n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
